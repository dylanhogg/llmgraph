{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JarYwM4FxV5x"
      },
      "source": [
        "# llmgraph\n",
        "\n",
        "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dylanhogg/llmgraph/blob/master/notebooks/llmgraph_example.ipynb)\n",
        "\n",
        "Create knowledge graphs with LLMs.\n",
        "\n",
        "https://github.com/dylanhogg/llmgraph\n",
        "\n",
        "<img src=\"https://github.com/dylanhogg/llmgraph/blob/main/docs/img/header.jpg?raw=true\" alt=\"drawing\" width=\"600px\"/>\n",
        "\n",
        "llmgraph enables you to create knowledge graphs in [GraphML](http://graphml.graphdrawing.org/), [GEXF](https://gexf.net/), and HTML formats (generated via [pyvis](https://github.com/WestHealth/pyvis)) from a given source entity Wikipedia page. The knowledge graphs are generated by extracting world knowledge from ChatGPT or other large language models (LLMs) as supported by [LiteLLM](https://github.com/BerriAI/litellm).\n",
        "\n",
        "For a background on knowledge graphs see a [youtube overview by Computerphile](https://www.youtube.com/watch?v=PZBm7M0HGzw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsrezNA9LppM"
      },
      "source": [
        "## Install llmgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Og6vjqOqxO9X"
      },
      "outputs": [],
      "source": [
        "# Install llmgraph from pypi (https://pypi.org/project/llmgraph/)\n",
        "# (Ignore any dependency resolver issues on Google Colab, they're fine)\n",
        "%pip install llmgraph -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_dwPBofNxP0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d515e3-6483-4e15-8412-8cd4f5f8c5a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llmgraph                         1.2.1\n"
          ]
        }
      ],
      "source": [
        "# Display installed llmgraph version\n",
        "%pip list | grep llmgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpVE4aQjLraA"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cN81mjiLzL5H"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "import os\n",
        "import getpass\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uuq7Q3AULwXx"
      },
      "source": [
        "## Enter your OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uXaylu7pzjqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f573ee2-b154-43c9-a555-6b1c4809a96d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key··········\n"
          ]
        }
      ],
      "source": [
        "# Set OPENAI_API_KEY from user input (hidden in UI via getpass function)\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gY_-vhLL4Uf"
      },
      "source": [
        "## Run llmgraph command"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!llmgraph --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-s6hACUKyeN",
        "outputId": "174747c5-0faa-4969-c85b-600b0249330b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m                                                                                                    \u001b[0m\n",
            "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1mllmgraph [OPTIONS] ENTITY_TYPE ENTITY_WIKIPEDIA\u001b[0m\u001b[1m                                            \u001b[0m\u001b[1m \u001b[0m\n",
            "\u001b[1m                                                                                                    \u001b[0m\n",
            " Create knowledge graphs with LLMs                                                                  \n",
            "                                                                                                    \n",
            "\u001b[2m╭─\u001b[0m\u001b[2m Arguments \u001b[0m\u001b[2m─────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m    entity_type           \u001b[1;33mTEXT\u001b[0m  Entity type (e.g. movie) \u001b[2m[default: None]\u001b[0m \u001b[2;31m[required]\u001b[0m             \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m    entity_wikipedia      \u001b[1;33mTEXT\u001b[0m  Full wikipedia link to root entity \u001b[2m[default: None]\u001b[0m \u001b[2;31m[required]\u001b[0m   \u001b[2m│\u001b[0m\n",
            "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-entity\u001b[0m\u001b[1;36m-root\u001b[0m                                      \u001b[1;33mTEXT                 \u001b[0m  Optional root entity   \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           name override if       \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           different from         \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           wikipedia page title   \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           \u001b[2m[default: None]       \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-levels\u001b[0m                                           \u001b[1;33mINTEGER              \u001b[0m  Number of levels deep  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           to construct from the  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           central root entity    \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           \u001b[2m[default: 2]          \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-max\u001b[0m\u001b[1;36m-sum-total-tokens\u001b[0m                             \u001b[1;33mINTEGER              \u001b[0m  Maximum sum of tokens  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           for graph generation   \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           \u001b[2m[default: 200000]     \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-output\u001b[0m\u001b[1;36m-folder\u001b[0m                                    \u001b[1;33mTEXT                 \u001b[0m  Folder location to     \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           write outputs          \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           \u001b[2m[default: ./_output/] \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-llm\u001b[0m\u001b[1;36m-model\u001b[0m                                        \u001b[1;33mTEXT                 \u001b[0m  The model name         \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           \u001b[2m[default:             \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           \u001b[2mgpt-3.5-turbo]        \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-llm\u001b[0m\u001b[1;36m-temp\u001b[0m                                         \u001b[1;33mFLOAT                \u001b[0m  LLM temperature value  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           \u001b[2m[default: 0.0]       \u001b[0m  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-llm\u001b[0m\u001b[1;36m-base-url\u001b[0m                                     \u001b[1;33mTEXT                 \u001b[0m  LLM will use custom    \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           base URL instead of    \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           the automatic one      \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           \u001b[2m[default: None]       \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-allow\u001b[0m\u001b[1;36m-user-input\u001b[0m        \u001b[1;35m-\u001b[0m\u001b[1;35m-no\u001b[0m\u001b[1;35m-allow-user-input\u001b[0m    \u001b[1;33m                     \u001b[0m  Allow command line     \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           user input             \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           \u001b[2m[default:             \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           \u001b[2mallow-user-input]     \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-version\u001b[0m                                          \u001b[1;33m                     \u001b[0m  Display llmgraph       \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           version                \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-install\u001b[0m\u001b[1;36m-completion\u001b[0m                               \u001b[1;2;33m[\u001b[0m\u001b[1;33mbash\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mzsh\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mfish\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mpowers\u001b[0m  Install completion for \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                    \u001b[1;33mhell\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mpwsh\u001b[0m\u001b[1;2;33m]\u001b[0m\u001b[1;33m           \u001b[0m  the specified shell.   \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           \u001b[2m[default: None]       \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-show\u001b[0m\u001b[1;36m-completion\u001b[0m                                  \u001b[1;2;33m[\u001b[0m\u001b[1;33mbash\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mzsh\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mfish\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mpowers\u001b[0m  Show completion for    \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                    \u001b[1;33mhell\u001b[0m\u001b[1;2;33m|\u001b[0m\u001b[1;33mpwsh\u001b[0m\u001b[1;2;33m]\u001b[0m\u001b[1;33m           \u001b[0m  the specified shell,   \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           to copy it or          \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           customize the          \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           installation.          \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           \u001b[2m[default: None]       \u001b[0m \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m                                             \u001b[1;33m                     \u001b[0m  Show this message and  \u001b[2m│\u001b[0m\n",
            "\u001b[2m│\u001b[0m                                                                           exit.                  \u001b[2m│\u001b[0m\n",
            "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wSux5AnAxP25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093e9602-d635-4092-b4aa-bfafbc64ce6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running with \u001b[33mentity_type\u001b[0m=\u001b[32m'concepts-general'\u001b[0m, \n",
            "\u001b[33mentity_wikipedia\u001b[0m=\u001b[32m'https://en.wikipedia.org/wiki/Large_language_model'\u001b[0m, \u001b[33mentity_root\u001b[0m=\u001b[32m'Large language \u001b[0m\n",
            "\u001b[32mmodel'\u001b[0m, \u001b[33mcustom_entity_root\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mlevels\u001b[0m=\u001b[1;36m3\u001b[0m, \u001b[33mllm_model\u001b[0m=\u001b[32m'gpt-3.5-turbo'\u001b[0m, \u001b[33mllm_temp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \n",
            "\u001b[33moutput_folder\u001b[0m=\u001b[32m'./_output/'\u001b[0m\n",
            "Reading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Large_language_model\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mLarge language model\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m1\u001b[0m, total tokens \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Transformer_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mmachine_learning_model\u001b[0m\u001b[1;4;32m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/BERT_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mlanguage_model\u001b[0m\u001b[1;4;32m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/GPT_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mlanguage_model\u001b[0m\u001b[1;4;32m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/XLNet_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mlanguage_model\u001b[0m\u001b[1;4;32m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/T5_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mlanguage_model\u001b[0m\u001b[1;4;32m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mTransformer \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mmachine learning model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m2\u001b[0m, total tokens \u001b[1;36m610\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/OpenAI#GPT_series\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/XLNet\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/GPT-3\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/T5_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mtext-to-text_transfer_transformer\u001b[0m\u001b[1;4;32m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mBERT \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m2\u001b[0m, total tokens \u001b[1;36m1\u001b[0m,\u001b[1;36m228\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/OpenAI#GPT\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/ELMo\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Word2vec\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/ULMFiT\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mGPT \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m2\u001b[0m, total tokens \u001b[1;36m1\u001b[0m,\u001b[1;36m850\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/OpenAI\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Transformer_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mneural_network\u001b[0m\u001b[1;4;32m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Language_modeling\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mXLNet \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m2\u001b[0m, total tokens \u001b[1;36m2\u001b[0m,\u001b[1;36m420\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/RoBERTa\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/ALBERT_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mlanguage_model\u001b[0m\u001b[1;4;32m)\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mT5 \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m2\u001b[0m, total tokens \u001b[1;36m2\u001b[0m,\u001b[1;36m995\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/OpenAI#GPT-3\u001b[0m\n",
            "\u001b[2KOutput html: \n",
            "\u001b[32m'_output/concepts-general/large-language-model/concepts-general_large-language-model_v1.2.1_level2_i\u001b[0m\n",
            "\u001b[32mncl_unprocessed.html'\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mGPT \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m3\u001b[0m,\u001b[1;36m505\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Recurrent_neural_network\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Deep_learning\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Natural_language_processing\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mXLNet \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m4\u001b[0m,\u001b[1;36m132\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mGPT-\u001b[0m\u001b[1;32m3\u001b[0m\u001b[1;32m \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m4\u001b[0m,\u001b[1;36m736\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/OpenAI#GPT-2\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Artificial_intelligence\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mT5 \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m5\u001b[0m,\u001b[1;36m220\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mGPT \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m5\u001b[0m,\u001b[1;36m765\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mELMo \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m6\u001b[0m,\u001b[1;36m333\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/FastText\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mWord2Vec \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mword embedding model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m6\u001b[0m,\u001b[1;36m850\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/GloVe_\u001b[0m\u001b[1;4;32m(\u001b[0m\u001b[1;4;32mmachine_learning\u001b[0m\u001b[1;4;32m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Doc2vec\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m404\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mULMFiT \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m7\u001b[0m,\u001b[1;36m377\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mOpenAI\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m7\u001b[0m,\u001b[1;36m944\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Machine_learning\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Reinforcement_learning\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mTransformer \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mneural network\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m8\u001b[0m,\u001b[1;36m400\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mLanguage modeling\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m8\u001b[0m,\u001b[1;36m881\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Statistical_language_modeling\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Machine_translation\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Speech_recognition\u001b[0m\n",
            "\u001b[2KReading \u001b[1;4;32mhttps://en.wikipedia.org/wiki/Information_retrieval\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mRoBERTa \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m9\u001b[0m,\u001b[1;36m397\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mALBERT \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mlanguage model\u001b[0m\u001b[1;32m)\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m9\u001b[0m,\u001b[1;36m919\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing \u001b[1;32mGPT-\u001b[0m\u001b[1;32m3\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m, total tokens \u001b[1;36m10\u001b[0m,\u001b[1;36m497\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KOutput html: \n",
            "\u001b[32m'_output/concepts-general/large-language-model/concepts-general_large-language-model_v1.2.1_level3_i\u001b[0m\n",
            "\u001b[32mncl_unprocessed.html'\u001b[0m \u001b[1m(\u001b[0mlevel \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\n",
            "\u001b[2KProcessing level 3: \u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3 \u001b[0m [ \u001b[33m0:02:28\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m? it/s\u001b[0m ]\n",
            "\u001b[?25h\n",
            "\u001b[1;32mllmgraph finished, took \u001b[0m\u001b[1;32m148.\u001b[0m\u001b[1;32m994283s.\u001b[0m\n",
            "Output written to folder \u001b[32m'_output/concepts-general/large-language-model'\u001b[0m which includes, for each \n",
            "level:\n",
            " - An html file with only processed nodes as a fully connected graph\n",
            " - An html file with both processed and extra unprocessed edge nodes\n",
            " - A .graphml file \u001b[1m(\u001b[0msee \u001b[4;94mhttp://graphml.graphdrawing.org/\u001b[0m\u001b[4;94m)\u001b[0m\n",
            " - A .gefx file, good for viewing in gephi \u001b[1m(\u001b[0msee \u001b[4;94mhttps://gexf.net/\u001b[0m and \u001b[4;94mhttps://gephi.org/\u001b[0m\u001b[4;94m)\u001b[0m\n",
            "\n",
            "Thank you for using llmgraph! Please consider starring the project on github: \n",
            "\u001b[4;94mhttps://github.com/dylanhogg/llmgraph\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Run llmgraph\n",
        "# Note: valid `entity_type` values are found here: https://github.com/dylanhogg/llmgraph/blob/main/llmgraph/prompts.yaml\n",
        "!llmgraph concepts-general https://en.wikipedia.org/wiki/Large_language_model --levels 3 --llm-model gpt-3.5-turbo --llm-temp 0.0 --no-allow-user-input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4-g6rk6L_B5"
      },
      "source": [
        "## Locate the output files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pDuAtHMq9ZaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b02dd3-4149-402e-e98f-dd7f278bd1c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_output/concepts-general/large-language-model/concepts-general_large-language-model_v1.2.1_level3_fully_connected.html\n",
            "_output/concepts-general/large-language-model/concepts-general_large-language-model_v1.2.1_level3.graphml\n"
          ]
        }
      ],
      "source": [
        "# Get list of book html files from the _output folder\n",
        "html_files = []\n",
        "graphml_files = []\n",
        "for root, dirs, files in os.walk(\"_output\"):\n",
        "  if not dirs:\n",
        "    html_files.extend([str(Path(root) / f) for f in files if f.endswith(\"fully_connected.html\")])\n",
        "    graphml_files.extend([str(Path(root) / f) for f in files if f.endswith(\".graphml\")])\n",
        "html_files = sorted(html_files)\n",
        "graphml_files = sorted(graphml_files)\n",
        "html_file = html_files[-1]\n",
        "graphml_file = graphml_files[-1]\n",
        "\n",
        "print(html_file)\n",
        "print(graphml_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z1UZdk2WJXWF"
      },
      "outputs": [],
      "source": [
        "# Uncomment these lines to download book html (or find it in the file tree on the left)\n",
        "# from google.colab import files\n",
        "# files.download(book_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display the network"
      ],
      "metadata": {
        "id": "RFixDPuJo11k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvis.network import Network"
      ],
      "metadata": {
        "id": "7egMzj8jfAV6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load graphml file\n",
        "G = nx.read_graphml(graphml_file)\n",
        "# G = nx.read_graphml(\"_output/concepts-general/large-language-model/concepts-general_large-language-model_v1.2.1_level3.graphml\")\n",
        "\n",
        "# Create pyvis network for displaying\n",
        "nt = Network(height=\"800px\", width=\"100%\", directed=True, cdn_resources=\"remote\", notebook=True)\n",
        "nt.from_nx(G)\n",
        "nt.force_atlas_2based(\n",
        "    spring_strength=0.03\n",
        ")"
      ],
      "metadata": {
        "id": "hPGvtkhTlV33"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display pyviz network\n",
        "nt.save_graph(\"llmgraph.html\")\n",
        "IPython.display.HTML(filename=\"llmgraph.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "p62WN_oclV8X",
        "outputId": "49d0e7e6-4f35-4314-9219-86ae31088e88"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<html>\n",
              "    <head>\n",
              "        <meta charset=\"utf-8\">\n",
              "        \n",
              "            <script>function neighbourhoodHighlight(params) {\n",
              "  // console.log(\"in nieghbourhoodhighlight\");\n",
              "  allNodes = nodes.get({ returnType: \"Object\" });\n",
              "  // originalNodes = JSON.parse(JSON.stringify(allNodes));\n",
              "  // if something is selected:\n",
              "  if (params.nodes.length > 0) {\n",
              "    highlightActive = true;\n",
              "    var i, j;\n",
              "    var selectedNode = params.nodes[0];\n",
              "    var degrees = 2;\n",
              "\n",
              "    // mark all nodes as hard to read.\n",
              "    for (let nodeId in allNodes) {\n",
              "      // nodeColors[nodeId] = allNodes[nodeId].color;\n",
              "      allNodes[nodeId].color = \"rgba(200,200,200,0.5)\";\n",
              "      if (allNodes[nodeId].hiddenLabel === undefined) {\n",
              "        allNodes[nodeId].hiddenLabel = allNodes[nodeId].label;\n",
              "        allNodes[nodeId].label = undefined;\n",
              "      }\n",
              "    }\n",
              "    var connectedNodes = network.getConnectedNodes(selectedNode);\n",
              "    var allConnectedNodes = [];\n",
              "\n",
              "    // get the second degree nodes\n",
              "    for (i = 1; i < degrees; i++) {\n",
              "      for (j = 0; j < connectedNodes.length; j++) {\n",
              "        allConnectedNodes = allConnectedNodes.concat(\n",
              "          network.getConnectedNodes(connectedNodes[j])\n",
              "        );\n",
              "      }\n",
              "    }\n",
              "\n",
              "    // all second degree nodes get a different color and their label back\n",
              "    for (i = 0; i < allConnectedNodes.length; i++) {\n",
              "      // allNodes[allConnectedNodes[i]].color = \"pink\";\n",
              "      allNodes[allConnectedNodes[i]].color = \"rgba(150,150,150,0.75)\";\n",
              "      if (allNodes[allConnectedNodes[i]].hiddenLabel !== undefined) {\n",
              "        allNodes[allConnectedNodes[i]].label =\n",
              "          allNodes[allConnectedNodes[i]].hiddenLabel;\n",
              "        allNodes[allConnectedNodes[i]].hiddenLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "\n",
              "    // all first degree nodes get their own color and their label back\n",
              "    for (i = 0; i < connectedNodes.length; i++) {\n",
              "      // allNodes[connectedNodes[i]].color = undefined;\n",
              "      allNodes[connectedNodes[i]].color = nodeColors[connectedNodes[i]];\n",
              "      if (allNodes[connectedNodes[i]].hiddenLabel !== undefined) {\n",
              "        allNodes[connectedNodes[i]].label =\n",
              "          allNodes[connectedNodes[i]].hiddenLabel;\n",
              "        allNodes[connectedNodes[i]].hiddenLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "\n",
              "    // the main node gets its own color and its label back.\n",
              "    // allNodes[selectedNode].color = undefined;\n",
              "    allNodes[selectedNode].color = nodeColors[selectedNode];\n",
              "    if (allNodes[selectedNode].hiddenLabel !== undefined) {\n",
              "      allNodes[selectedNode].label = allNodes[selectedNode].hiddenLabel;\n",
              "      allNodes[selectedNode].hiddenLabel = undefined;\n",
              "    }\n",
              "  } else if (highlightActive === true) {\n",
              "    // console.log(\"highlightActive was true\");\n",
              "    // reset all nodes\n",
              "    for (let nodeId in allNodes) {\n",
              "      // allNodes[nodeId].color = \"purple\";\n",
              "      allNodes[nodeId].color = nodeColors[nodeId];\n",
              "      // delete allNodes[nodeId].color;\n",
              "      if (allNodes[nodeId].hiddenLabel !== undefined) {\n",
              "        allNodes[nodeId].label = allNodes[nodeId].hiddenLabel;\n",
              "        allNodes[nodeId].hiddenLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "    highlightActive = false;\n",
              "  }\n",
              "\n",
              "  // transform the object into an array\n",
              "  var updateArray = [];\n",
              "  if (params.nodes.length > 0) {\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes.hasOwnProperty(nodeId)) {\n",
              "        // console.log(allNodes[nodeId]);\n",
              "        updateArray.push(allNodes[nodeId]);\n",
              "      }\n",
              "    }\n",
              "    nodes.update(updateArray);\n",
              "  } else {\n",
              "    // console.log(\"Nothing was selected\");\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes.hasOwnProperty(nodeId)) {\n",
              "        // console.log(allNodes[nodeId]);\n",
              "        // allNodes[nodeId].color = {};\n",
              "        updateArray.push(allNodes[nodeId]);\n",
              "      }\n",
              "    }\n",
              "    nodes.update(updateArray);\n",
              "  }\n",
              "}\n",
              "\n",
              "function filterHighlight(params) {\n",
              "  allNodes = nodes.get({ returnType: \"Object\" });\n",
              "  // if something is selected:\n",
              "  if (params.nodes.length > 0) {\n",
              "    filterActive = true;\n",
              "    let selectedNodes = params.nodes;\n",
              "\n",
              "    // hiding all nodes and saving the label\n",
              "    for (let nodeId in allNodes) {\n",
              "      allNodes[nodeId].hidden = true;\n",
              "      if (allNodes[nodeId].savedLabel === undefined) {\n",
              "        allNodes[nodeId].savedLabel = allNodes[nodeId].label;\n",
              "        allNodes[nodeId].label = undefined;\n",
              "      }\n",
              "    }\n",
              "\n",
              "    for (let i=0; i < selectedNodes.length; i++) {\n",
              "      allNodes[selectedNodes[i]].hidden = false;\n",
              "      if (allNodes[selectedNodes[i]].savedLabel !== undefined) {\n",
              "        allNodes[selectedNodes[i]].label = allNodes[selectedNodes[i]].savedLabel;\n",
              "        allNodes[selectedNodes[i]].savedLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "\n",
              "  } else if (filterActive === true) {\n",
              "    // reset all nodes\n",
              "    for (let nodeId in allNodes) {\n",
              "      allNodes[nodeId].hidden = false;\n",
              "      if (allNodes[nodeId].savedLabel !== undefined) {\n",
              "        allNodes[nodeId].label = allNodes[nodeId].savedLabel;\n",
              "        allNodes[nodeId].savedLabel = undefined;\n",
              "      }\n",
              "    }\n",
              "    filterActive = false;\n",
              "  }\n",
              "\n",
              "  // transform the object into an array\n",
              "  var updateArray = [];\n",
              "  if (params.nodes.length > 0) {\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes.hasOwnProperty(nodeId)) {\n",
              "        updateArray.push(allNodes[nodeId]);\n",
              "      }\n",
              "    }\n",
              "    nodes.update(updateArray);\n",
              "  } else {\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes.hasOwnProperty(nodeId)) {\n",
              "        updateArray.push(allNodes[nodeId]);\n",
              "      }\n",
              "    }\n",
              "    nodes.update(updateArray);\n",
              "  }\n",
              "}\n",
              "\n",
              "function selectNode(nodes) {\n",
              "  network.selectNodes(nodes);\n",
              "  neighbourhoodHighlight({ nodes: nodes });\n",
              "  return nodes;\n",
              "}\n",
              "\n",
              "function selectNodes(nodes) {\n",
              "  network.selectNodes(nodes);\n",
              "  filterHighlight({nodes: nodes});\n",
              "  return nodes;\n",
              "}\n",
              "\n",
              "function highlightFilter(filter) {\n",
              "  let selectedNodes = []\n",
              "  let selectedProp = filter['property']\n",
              "  if (filter['item'] === 'node') {\n",
              "    let allNodes = nodes.get({ returnType: \"Object\" });\n",
              "    for (let nodeId in allNodes) {\n",
              "      if (allNodes[nodeId][selectedProp] && filter['value'].includes((allNodes[nodeId][selectedProp]).toString())) {\n",
              "        selectedNodes.push(nodeId)\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "  else if (filter['item'] === 'edge'){\n",
              "    let allEdges = edges.get({returnType: 'object'});\n",
              "    // check if the selected property exists for selected edge and select the nodes connected to the edge\n",
              "    for (let edge in allEdges) {\n",
              "      if (allEdges[edge][selectedProp] && filter['value'].includes((allEdges[edge][selectedProp]).toString())) {\n",
              "        selectedNodes.push(allEdges[edge]['from'])\n",
              "        selectedNodes.push(allEdges[edge]['to'])\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "  selectNodes(selectedNodes)\n",
              "}</script>\n",
              "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css\" integrity=\"sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\" />\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js\" integrity=\"sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n",
              "            \n",
              "            \n",
              "            \n",
              "            \n",
              "            \n",
              "            \n",
              "\n",
              "        \n",
              "<center>\n",
              "<h1></h1>\n",
              "</center>\n",
              "\n",
              "<!-- <link rel=\"stylesheet\" href=\"../node_modules/vis/dist/vis.min.css\" type=\"text/css\" />\n",
              "<script type=\"text/javascript\" src=\"../node_modules/vis/dist/vis.js\"> </script>-->\n",
              "        <link\n",
              "          href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css\"\n",
              "          rel=\"stylesheet\"\n",
              "          integrity=\"sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6\"\n",
              "          crossorigin=\"anonymous\"\n",
              "        />\n",
              "        <script\n",
              "          src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js\"\n",
              "          integrity=\"sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf\"\n",
              "          crossorigin=\"anonymous\"\n",
              "        ></script>\n",
              "\n",
              "\n",
              "        <center>\n",
              "          <h1></h1>\n",
              "        </center>\n",
              "        <style type=\"text/css\">\n",
              "\n",
              "             #mynetwork {\n",
              "                 width: 100%;\n",
              "                 height: 800px;\n",
              "                 background-color: #ffffff;\n",
              "                 border: 1px solid lightgray;\n",
              "                 position: relative;\n",
              "                 float: left;\n",
              "             }\n",
              "\n",
              "             \n",
              "\n",
              "             \n",
              "\n",
              "             \n",
              "             /* position absolute is important and the container has to be relative or absolute as well. */\n",
              "          div.popup {\n",
              "                 position:absolute;\n",
              "                 top:0px;\n",
              "                 left:0px;\n",
              "                 display:none;\n",
              "                 background-color:#f5f4ed;\n",
              "                 -moz-border-radius: 3px;\n",
              "                 -webkit-border-radius: 3px;\n",
              "                 border-radius: 3px;\n",
              "                 border: 1px solid #808074;\n",
              "                 box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.2);\n",
              "          }\n",
              "\n",
              "          /* hide the original tooltip */\n",
              "          .vis-tooltip {\n",
              "            display:none;\n",
              "          }\n",
              "             \n",
              "        </style>\n",
              "    </head>\n",
              "\n",
              "\n",
              "    <body>\n",
              "        <div class=\"card\" style=\"width: 100%\">\n",
              "            \n",
              "            \n",
              "            <div id=\"mynetwork\" class=\"card-body\"></div>\n",
              "        </div>\n",
              "\n",
              "        \n",
              "        \n",
              "\n",
              "        <script type=\"text/javascript\">\n",
              "\n",
              "              // initialize global variables.\n",
              "              var edges;\n",
              "              var nodes;\n",
              "              var allNodes;\n",
              "              var allEdges;\n",
              "              var nodeColors;\n",
              "              var originalNodes;\n",
              "              var network;\n",
              "              var container;\n",
              "              var options, data;\n",
              "              var filter = {\n",
              "                  item : '',\n",
              "                  property : '',\n",
              "                  value : []\n",
              "              };\n",
              "\n",
              "              \n",
              "\n",
              "              \n",
              "\n",
              "              // This method is responsible for drawing the graph, returns the drawn network\n",
              "              function drawGraph() {\n",
              "                  var container = document.getElementById('mynetwork');\n",
              "\n",
              "                  \n",
              "\n",
              "                  // parsing and collecting nodes and edges from the python\n",
              "                  nodes = new vis.DataSet([{\"group\": 1, \"id\": \"Large language model\", \"label\": \"Large language model\", \"level\": 1, \"name\": \"Large language model\", \"node_count\": 0, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"0. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Large_language_model\\u0027 target=\\u0027_blank\\u0027\\u003eLarge language model\\u003c/a\\u003e\\u003cbr /\\u003eA large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs are artificial neural networks, the largest and most capable of which are built with a transformer-based architecture. Some recent implementations are based on other architectures, such as recurrent\\u003cbr /\\u003e[200, G1, L1, PR]\", \"wikipedia_canonical\": \"Large_language_model\", \"wikipedia_content\": \"A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs are artificial neural networks, the largest and most capable of which are built with a transformer-based architecture. Some recent implementations are based on other architectures, such as recurrent\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Large_language_model\", \"wikipedia_normalized\": \"Large language model\", \"wikipedia_resp_code\": 200}, {\"group\": 2, \"id\": \"Transformer (machine learning model)\", \"label\": \"Transformer (machine learning model)\", \"level\": 2, \"name\": \"Transformer (machine learning model)\", \"node_count\": 1, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"1. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\\u0027 target=\\u0027_blank\\u0027\\u003eTransformer (machine learning model)\\u003c/a\\u003e\\u003cbr /\\u003eA transformer is a deep learning architecture based on the multi-head attention mechanism, proposed in a 2017 paper \\\"Attention Is All You Need\\\". It has no recurrent units, and thus requires less training time than previous recurrent neural architectures, such as long short-term memory (LSTM), and its later variation has been prevalently adopted for training large language models on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\\nInput text is split into n-grams encoded \\u003cbr /\\u003e[200, G2, L2, PR]\", \"wikipedia_canonical\": \"Transformer_(machine_learning_model)\", \"wikipedia_content\": \"A transformer is a deep learning architecture based on the multi-head attention mechanism, proposed in a 2017 paper \\\"Attention Is All You Need\\\". It has no recurrent units, and thus requires less training time than previous recurrent neural architectures, such as long short-term memory (LSTM), and its later variation has been prevalently adopted for training large language models on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\\nInput text is split into n-grams encoded \", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\", \"wikipedia_normalized\": \"Transformer (machine learning model)\", \"wikipedia_resp_code\": 200}, {\"group\": 2, \"id\": \"BERT (language model)\", \"label\": \"BERT (language model)\", \"level\": 2, \"name\": \"BERT (language model)\", \"node_count\": 2, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"2. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/BERT_(language_model)\\u0027 target=\\u0027_blank\\u0027\\u003eBERT (language model)\\u003c/a\\u003e\\u003cbr /\\u003e \\nBidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that \\\"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\\\"\\u003cbr /\\u003e[200, G2, L2, PR]\", \"wikipedia_canonical\": \"BERT_(language_model)\", \"wikipedia_content\": \" \\nBidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that \\\"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\\\"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/BERT_(language_model)\", \"wikipedia_normalized\": \"BERT (language model)\", \"wikipedia_resp_code\": 200}, {\"group\": 2, \"id\": \"GPT (language model)\", \"label\": \"GPT (language model)\", \"level\": 2, \"name\": \"GPT (language model)\", \"node_count\": 3, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"3. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/GPT_(language_model)\\u0027 target=\\u0027_blank\\u0027\\u003eGPT (language model)\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\\u0027 target=\\u0027_blank\\u0027\\u003eGenerative pre-trained transformer\\u003c/a\\u003e\\u003cbr /\\u003eGenerative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content. As of 2023, most LLMs have these characteristics and are sometimes referred to broadly as GPTs.\\u003cbr /\\u003e[200, G2, L2, PR]\", \"wikipedia_canonical\": \"Generative_pre-trained_transformer\", \"wikipedia_content\": \"Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content. As of 2023, most LLMs have these characteristics and are sometimes referred to broadly as GPTs.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/GPT_(language_model)\", \"wikipedia_normalized\": \"Generative pre-trained transformer\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"XLNet (language model)\", \"label\": \"XLNet (language model)\", \"level\": 2, \"name\": \"XLNet (language model)\", \"node_count\": 4, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"4. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/XLNet_(language_model)\\u0027 target=\\u0027_blank\\u0027\\u003eXLNet (language model)\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L2, PR]\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/XLNet_(language_model)\", \"wikipedia_resp_code\": 404}, {\"group\": 500, \"id\": \"T5 (language model)\", \"label\": \"T5 (language model)\", \"level\": 2, \"name\": \"T5 (language model)\", \"node_count\": 5, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"5. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/T5_(language_model)\\u0027 target=\\u0027_blank\\u0027\\u003eT5 (language model)\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L2, PR]\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/T5_(language_model)\", \"wikipedia_resp_code\": 404}, {\"group\": 3, \"id\": \"OpenAI#GPT series\", \"label\": \"OpenAI#GPT series\", \"level\": 3, \"name\": \"GPT (language model)\", \"node_count\": 6, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"6. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/OpenAI#GPT_series\\u0027 target=\\u0027_blank\\u0027\\u003eOpenAI#GPT series\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/OpenAI\\u0027 target=\\u0027_blank\\u0027\\u003eOpenAI\\u003c/a\\u003e\\u003cbr /\\u003eOpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \\\"safe and beneficial\\\" artificial general intelligence, which it defines as \\\"highly autonomous systems that outperform humans at most economically valuable work\\\".\\nAs one of the leading organizations of the AI Spring, it has developed several large language models, advanced image generation models, and previously, released open-source m\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"OpenAI\", \"wikipedia_content\": \"OpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \\\"safe and beneficial\\\" artificial general intelligence, which it defines as \\\"highly autonomous systems that outperform humans at most economically valuable work\\\".\\nAs one of the leading organizations of the AI Spring, it has developed several large language models, advanced image generation models, and previously, released open-source m\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/OpenAI#GPT_series\", \"wikipedia_normalized\": \"OpenAI\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"XLNet\", \"label\": \"XLNet\", \"level\": 3, \"name\": \"XLNet (language model)\", \"node_count\": 7, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"7. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/XLNet\\u0027 target=\\u0027_blank\\u0027\\u003eXLNet\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L3, PR]\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/XLNet\", \"wikipedia_resp_code\": 404}, {\"group\": 3, \"id\": \"GPT-3\", \"label\": \"GPT-3\", \"level\": 3, \"name\": \"GPT-3 (language model)\", \"node_count\": 8, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"8. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/GPT-3\\u0027 target=\\u0027_blank\\u0027\\u003eGPT-3\\u003c/a\\u003e\\u003cbr /\\u003eGenerative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. Like its predecessor, GPT-2, it is a decoder-only transformer model of deep neural network, which supersedes recurrence and convolution-based architectures with a technique known as \\\"attention\\\". This attention mechanism allows the model to selectively focus on segments of input text it predicts to be most relevant. It uses a 2048-tokens-long context, float16 (16-bit) precision, and a hitherto-unpre\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"GPT-3\", \"wikipedia_content\": \"Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. Like its predecessor, GPT-2, it is a decoder-only transformer model of deep neural network, which supersedes recurrence and convolution-based architectures with a technique known as \\\"attention\\\". This attention mechanism allows the model to selectively focus on segments of input text it predicts to be most relevant. It uses a 2048-tokens-long context, float16 (16-bit) precision, and a hitherto-unpre\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/GPT-3\", \"wikipedia_normalized\": \"GPT-3\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"T5 (text-to-text transfer transformer)\", \"label\": \"T5 (text-to-text transfer transformer)\", \"level\": 3, \"name\": \"T5 (language model)\", \"node_count\": 9, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"9. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/T5_(text-to-text_transfer_transformer)\\u0027 target=\\u0027_blank\\u0027\\u003eT5 (text-to-text transfer transformer)\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L3, PR]\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/T5_(text-to-text_transfer_transformer)\", \"wikipedia_resp_code\": 404}, {\"group\": 3, \"id\": \"OpenAI#GPT\", \"label\": \"OpenAI#GPT\", \"level\": 3, \"name\": \"GPT (language model)\", \"node_count\": 10, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"10. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/OpenAI#GPT\\u0027 target=\\u0027_blank\\u0027\\u003eOpenAI#GPT\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/OpenAI\\u0027 target=\\u0027_blank\\u0027\\u003eOpenAI\\u003c/a\\u003e\\u003cbr /\\u003eOpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \\\"safe and beneficial\\\" artificial general intelligence, which it defines as \\\"highly autonomous systems that outperform humans at most economically valuable work\\\".\\nAs one of the leading organizations of the AI Spring, it has developed several large language models, advanced image generation models, and previously, released open-source m\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"OpenAI\", \"wikipedia_content\": \"OpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \\\"safe and beneficial\\\" artificial general intelligence, which it defines as \\\"highly autonomous systems that outperform humans at most economically valuable work\\\".\\nAs one of the leading organizations of the AI Spring, it has developed several large language models, advanced image generation models, and previously, released open-source m\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/OpenAI#GPT\", \"wikipedia_normalized\": \"OpenAI\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"ELMo\", \"label\": \"ELMo\", \"level\": 3, \"name\": \"ELMo (language model)\", \"node_count\": 11, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"11. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/ELMo\\u0027 target=\\u0027_blank\\u0027\\u003eELMo\\u003c/a\\u003e\\u003cbr /\\u003eELMo is a word embedding method for representing a sequence of words as a corresponding sequence of vectors. Character-level tokens are taken as the inputs to a bidirectional LSTM which produces word-level embeddings. Like BERT, ELMo embeddings are context-sensitive, producing different representations for words that share the same spelling but have different meanings (homonyms) such as \\\"bank\\\" in \\\"river bank\\\" and \\\"bank balance\\\".\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"ELMo\", \"wikipedia_content\": \"ELMo is a word embedding method for representing a sequence of words as a corresponding sequence of vectors. Character-level tokens are taken as the inputs to a bidirectional LSTM which produces word-level embeddings. Like BERT, ELMo embeddings are context-sensitive, producing different representations for words that share the same spelling but have different meanings (homonyms) such as \\\"bank\\\" in \\\"river bank\\\" and \\\"bank balance\\\".\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/ELMo\", \"wikipedia_normalized\": \"ELMo\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Word2vec\", \"label\": \"Word2vec\", \"level\": 3, \"name\": \"Word2Vec (word embedding model)\", \"node_count\": 12, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"12. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Word2vec\\u0027 target=\\u0027_blank\\u0027\\u003eWord2vec\\u003c/a\\u003e\\u003cbr /\\u003eWord2vec is a technique in natural language processing (NLP) for obtaining vector representations of words. These vectors capture information about the meaning of the word and their usage in context. The word2vec algorithm estimates these representations by modeling text in a large corpus. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Word2vec\", \"wikipedia_content\": \"Word2vec is a technique in natural language processing (NLP) for obtaining vector representations of words. These vectors capture information about the meaning of the word and their usage in context. The word2vec algorithm estimates these representations by modeling text in a large corpus. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Word2vec\", \"wikipedia_normalized\": \"Word2vec\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"ULMFiT\", \"label\": \"ULMFiT\", \"level\": 3, \"name\": \"ULMFiT (language model)\", \"node_count\": 13, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"13. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/ULMFiT\\u0027 target=\\u0027_blank\\u0027\\u003eULMFiT\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L3, PR]\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/ULMFiT\", \"wikipedia_resp_code\": 404}, {\"group\": 3, \"id\": \"OpenAI\", \"label\": \"OpenAI\", \"level\": 3, \"name\": \"OpenAI\", \"node_count\": 14, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"14. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/OpenAI\\u0027 target=\\u0027_blank\\u0027\\u003eOpenAI\\u003c/a\\u003e\\u003cbr /\\u003eOpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \\\"safe and beneficial\\\" artificial general intelligence, which it defines as \\\"highly autonomous systems that outperform humans at most economically valuable work\\\".\\nAs one of the leading organizations of the AI Spring, it has developed several large language models, advanced image generation models, and previously, released open-source m\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"OpenAI\", \"wikipedia_content\": \"OpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \\\"safe and beneficial\\\" artificial general intelligence, which it defines as \\\"highly autonomous systems that outperform humans at most economically valuable work\\\".\\nAs one of the leading organizations of the AI Spring, it has developed several large language models, advanced image generation models, and previously, released open-source m\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/OpenAI\", \"wikipedia_normalized\": \"OpenAI\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Transformer (neural network)\", \"label\": \"Transformer (neural network)\", \"level\": 3, \"name\": \"Transformer (neural network)\", \"node_count\": 15, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"15. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Transformer_(neural_network)\\u0027 target=\\u0027_blank\\u0027\\u003eTransformer (neural network)\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\\u0027 target=\\u0027_blank\\u0027\\u003eTransformer (machine learning model)\\u003c/a\\u003e\\u003cbr /\\u003eA transformer is a deep learning architecture based on the multi-head attention mechanism, proposed in a 2017 paper \\\"Attention Is All You Need\\\". It has no recurrent units, and thus requires less training time than previous recurrent neural architectures, such as long short-term memory (LSTM), and its later variation has been prevalently adopted for training large language models on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\\nInput text is split into n-grams encoded \\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Transformer_(machine_learning_model)\", \"wikipedia_content\": \"A transformer is a deep learning architecture based on the multi-head attention mechanism, proposed in a 2017 paper \\\"Attention Is All You Need\\\". It has no recurrent units, and thus requires less training time than previous recurrent neural architectures, such as long short-term memory (LSTM), and its later variation has been prevalently adopted for training large language models on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\\nInput text is split into n-grams encoded \", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Transformer_(neural_network)\", \"wikipedia_normalized\": \"Transformer (machine learning model)\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Language modeling\", \"label\": \"Language modeling\", \"level\": 3, \"name\": \"Language modeling\", \"node_count\": 16, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"16. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Language_modeling\\u0027 target=\\u0027_blank\\u0027\\u003eLanguage modeling\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Language_model\\u0027 target=\\u0027_blank\\u0027\\u003eLanguage model\\u003c/a\\u003e\\u003cbr /\\u003eA language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed \\u2018Shannon-style\\u2019 experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Language_model\", \"wikipedia_content\": \"A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed \\u2018Shannon-style\\u2019 experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Language_modeling\", \"wikipedia_normalized\": \"Language model\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"RoBERTa\", \"label\": \"RoBERTa\", \"level\": 3, \"name\": \"RoBERTa (language model)\", \"node_count\": 17, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"17. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/RoBERTa\\u0027 target=\\u0027_blank\\u0027\\u003eRoBERTa\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L3, PR]\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/RoBERTa\", \"wikipedia_resp_code\": 404}, {\"group\": 500, \"id\": \"ALBERT (language model)\", \"label\": \"ALBERT (language model)\", \"level\": 3, \"name\": \"ALBERT (language model)\", \"node_count\": 18, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"18. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/ALBERT_(language_model)\\u0027 target=\\u0027_blank\\u0027\\u003eALBERT (language model)\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L3, PR]\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/ALBERT_(language_model)\", \"wikipedia_resp_code\": 404}, {\"group\": 3, \"id\": \"OpenAI#GPT-3\", \"label\": \"OpenAI#GPT-3\", \"level\": 3, \"name\": \"GPT-3\", \"node_count\": 19, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"19. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/OpenAI#GPT-3\\u0027 target=\\u0027_blank\\u0027\\u003eOpenAI#GPT-3\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/OpenAI\\u0027 target=\\u0027_blank\\u0027\\u003eOpenAI\\u003c/a\\u003e\\u003cbr /\\u003eOpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \\\"safe and beneficial\\\" artificial general intelligence, which it defines as \\\"highly autonomous systems that outperform humans at most economically valuable work\\\".\\nAs one of the leading organizations of the AI Spring, it has developed several large language models, advanced image generation models, and previously, released open-source m\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"OpenAI\", \"wikipedia_content\": \"OpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \\\"safe and beneficial\\\" artificial general intelligence, which it defines as \\\"highly autonomous systems that outperform humans at most economically valuable work\\\".\\nAs one of the leading organizations of the AI Spring, it has developed several large language models, advanced image generation models, and previously, released open-source m\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/OpenAI#GPT-3\", \"wikipedia_normalized\": \"OpenAI\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Recurrent neural network\", \"label\": \"Recurrent neural network\", \"level\": 4, \"name\": \"Recurrent neural network\", \"node_count\": 20, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"20. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Recurrent_neural_network\\u0027 target=\\u0027_blank\\u0027\\u003eRecurrent neural network\\u003c/a\\u003e\\u003cbr /\\u003eA recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Recurrent_neural_network\", \"wikipedia_content\": \"A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Recurrent_neural_network\", \"wikipedia_normalized\": \"Recurrent neural network\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Deep learning\", \"label\": \"Deep learning\", \"level\": 4, \"name\": \"Deep learning\", \"node_count\": 21, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"21. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Deep_learning\\u0027 target=\\u0027_blank\\u0027\\u003eDeep learning\\u003c/a\\u003e\\u003cbr /\\u003eDeep learning is the subset of machine learning methods based on artificial neural networks with representation learning. The adjective \\\"deep\\\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Deep_learning\", \"wikipedia_content\": \"Deep learning is the subset of machine learning methods based on artificial neural networks with representation learning. The adjective \\\"deep\\\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Deep_learning\", \"wikipedia_normalized\": \"Deep learning\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Natural language processing\", \"label\": \"Natural language processing\", \"level\": 4, \"name\": \"Natural language processing\", \"node_count\": 22, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"22. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Natural_language_processing\\u0027 target=\\u0027_blank\\u0027\\u003eNatural language processing\\u003c/a\\u003e\\u003cbr /\\u003eNatural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic machine learning approaches. The goal is a computer capable of \\\"understanding\\\" the contents of documents, including the contextual nuances of the language within t\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Natural_language_processing\", \"wikipedia_content\": \"Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic machine learning approaches. The goal is a computer capable of \\\"understanding\\\" the contents of documents, including the contextual nuances of the language within t\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Natural_language_processing\", \"wikipedia_normalized\": \"Natural language processing\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"OpenAI#GPT-2\", \"label\": \"OpenAI#GPT-2\", \"level\": 4, \"name\": \"GPT-2\", \"node_count\": 23, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"23. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/OpenAI#GPT-2\\u0027 target=\\u0027_blank\\u0027\\u003eOpenAI#GPT-2\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/OpenAI\\u0027 target=\\u0027_blank\\u0027\\u003eOpenAI\\u003c/a\\u003e\\u003cbr /\\u003eOpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \\\"safe and beneficial\\\" artificial general intelligence, which it defines as \\\"highly autonomous systems that outperform humans at most economically valuable work\\\".\\nAs one of the leading organizations of the AI Spring, it has developed several large language models, advanced image generation models, and previously, released open-source m\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"OpenAI\", \"wikipedia_content\": \"OpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \\\"safe and beneficial\\\" artificial general intelligence, which it defines as \\\"highly autonomous systems that outperform humans at most economically valuable work\\\".\\nAs one of the leading organizations of the AI Spring, it has developed several large language models, advanced image generation models, and previously, released open-source m\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/OpenAI#GPT-2\", \"wikipedia_normalized\": \"OpenAI\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Artificial intelligence\", \"label\": \"Artificial intelligence\", \"level\": 4, \"name\": \"Artificial intelligence\", \"node_count\": 24, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"24. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Artificial_intelligence\\u0027 target=\\u0027_blank\\u0027\\u003eArtificial intelligence\\u003c/a\\u003e\\u003cbr /\\u003eArtificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of humans or other animals. It is a field of study in computer science that develops and studies intelligent machines. Such machines may be called AIs.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Artificial_intelligence\", \"wikipedia_content\": \"Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of humans or other animals. It is a field of study in computer science that develops and studies intelligent machines. Such machines may be called AIs.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\", \"wikipedia_normalized\": \"Artificial intelligence\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"FastText\", \"label\": \"FastText\", \"level\": 4, \"name\": \"FastText\", \"node_count\": 25, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"25. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/FastText\\u0027 target=\\u0027_blank\\u0027\\u003eFastText\\u003c/a\\u003e\\u003cbr /\\u003efastText is a library for learning of word embeddings and text classification created by Facebook\\u0027s AI Research (FAIR) lab. The model allows one to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages. Several papers describe the techniques used by fastText.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"FastText\", \"wikipedia_content\": \"fastText is a library for learning of word embeddings and text classification created by Facebook\\u0027s AI Research (FAIR) lab. The model allows one to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages. Several papers describe the techniques used by fastText.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/FastText\", \"wikipedia_normalized\": \"FastText\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"GloVe (machine learning)\", \"label\": \"GloVe (machine learning)\", \"level\": 4, \"name\": \"GloVe\", \"node_count\": 26, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"26. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/GloVe_(machine_learning)\\u0027 target=\\u0027_blank\\u0027\\u003eGloVe (machine learning)\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/GloVe\\u0027 target=\\u0027_blank\\u0027\\u003eGloVe\\u003c/a\\u003e\\u003cbr /\\u003eGloVe, coined from Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"GloVe\", \"wikipedia_content\": \"GloVe, coined from Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/GloVe_(machine_learning)\", \"wikipedia_normalized\": \"GloVe\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"Doc2vec\", \"label\": \"Doc2vec\", \"level\": 4, \"name\": \"Doc2vec\", \"node_count\": 27, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"27. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Doc2vec\\u0027 target=\\u0027_blank\\u0027\\u003eDoc2vec\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Doc2vec\", \"wikipedia_resp_code\": 404}, {\"group\": 4, \"id\": \"Machine learning\", \"label\": \"Machine learning\", \"level\": 4, \"name\": \"Machine learning\", \"node_count\": 28, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"28. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Machine_learning\\u0027 target=\\u0027_blank\\u0027\\u003eMachine learning\\u003c/a\\u003e\\u003cbr /\\u003eMachine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Machine_learning\", \"wikipedia_content\": \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Machine_learning\", \"wikipedia_normalized\": \"Machine learning\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Reinforcement learning\", \"label\": \"Reinforcement learning\", \"level\": 4, \"name\": \"Reinforcement learning\", \"node_count\": 29, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"29. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Reinforcement_learning\\u0027 target=\\u0027_blank\\u0027\\u003eReinforcement learning\\u003c/a\\u003e\\u003cbr /\\u003eReinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Reinforcement_learning\", \"wikipedia_content\": \"Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Reinforcement_learning\", \"wikipedia_normalized\": \"Reinforcement learning\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Statistical language modeling\", \"label\": \"Statistical language modeling\", \"level\": 4, \"name\": \"Statistical language modeling\", \"node_count\": 30, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"30. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Statistical_language_modeling\\u0027 target=\\u0027_blank\\u0027\\u003eStatistical language modeling\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Language_model\\u0027 target=\\u0027_blank\\u0027\\u003eLanguage model\\u003c/a\\u003e\\u003cbr /\\u003eA language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed \\u2018Shannon-style\\u2019 experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Language_model\", \"wikipedia_content\": \"A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed \\u2018Shannon-style\\u2019 experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Statistical_language_modeling\", \"wikipedia_normalized\": \"Language model\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Machine translation\", \"label\": \"Machine translation\", \"level\": 4, \"name\": \"Machine translation\", \"node_count\": 31, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"31. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Machine_translation\\u0027 target=\\u0027_blank\\u0027\\u003eMachine translation\\u003c/a\\u003e\\u003cbr /\\u003eMachine translation is use of either rule-based or probabilistic machine learning approaches to translation of text or speech from one language to another, including the contextual, idiomatic and pragmatic nuances of both languages.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Machine_translation\", \"wikipedia_content\": \"Machine translation is use of either rule-based or probabilistic machine learning approaches to translation of text or speech from one language to another, including the contextual, idiomatic and pragmatic nuances of both languages.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Machine_translation\", \"wikipedia_normalized\": \"Machine translation\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Speech recognition\", \"label\": \"Speech recognition\", \"level\": 4, \"name\": \"Speech recognition\", \"node_count\": 32, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"32. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Speech_recognition\\u0027 target=\\u0027_blank\\u0027\\u003eSpeech recognition\\u003c/a\\u003e\\u003cbr /\\u003e\\nSpeech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Speech_recognition\", \"wikipedia_content\": \"\\nSpeech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Speech_recognition\", \"wikipedia_normalized\": \"Speech recognition\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Information retrieval\", \"label\": \"Information retrieval\", \"level\": 4, \"name\": \"Information retrieval\", \"node_count\": 33, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"33. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Information_retrieval\\u0027 target=\\u0027_blank\\u0027\\u003eInformation retrieval\\u003c/a\\u003e\\u003cbr /\\u003eInformation retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need. The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Information_retrieval\", \"wikipedia_content\": \"Information retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need. The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Information_retrieval\", \"wikipedia_normalized\": \"Information retrieval\", \"wikipedia_resp_code\": 200}]);\n",
              "                  edges = new vis.DataSet([{\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"Both Large language models and Transformers are machine learning models commonly used in natural language processing tasks. Transformers are often used as the underlying architecture for large language models.\", \"similarity\": 0.9, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"BERT (Bidirectional Encoder Representations from Transformers) is a large language model that utilizes the Transformer architecture. It is similar to Large language models in terms of being a state-of-the-art language model.\", \"similarity\": 0.8, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"GPT (Generative Pre-trained Transformer) is another example of a large language model that uses the Transformer architecture. It shares similarities with Large language models in terms of being pre-trained on large amounts of text data and generating coherent text.\", \"similarity\": 0.7, \"to\": \"GPT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"XLNet is a large language model that extends the Transformer architecture by incorporating permutation-based training. It is similar to Large language models in terms of being a powerful language model used for various natural language processing tasks.\", \"similarity\": 0.6, \"to\": \"XLNet (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"T5 (Text-to-Text Transfer Transformer) is a large language model that follows the Transformer architecture. It is similar to Large language models in terms of being a versatile language model capable of performing various text-to-text tasks.\", \"similarity\": 0.5, \"to\": \"T5 (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"Both BERT and Transformer are popular machine learning models based on the Transformer architecture. BERT is a pre-trained language model that uses the Transformer to understand the context of words in a sentence.\", \"similarity\": 0.9, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"GPT (Generative Pre-trained Transformer) is another language model that utilizes the Transformer architecture. It is similar to Transformer in terms of its architecture and ability to generate coherent text based on input prompts.\", \"similarity\": 0.85, \"to\": \"OpenAI#GPT series\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"XLNet is a language model that extends the Transformer architecture by incorporating permutation-based training. It shares similarities with Transformer in terms of its underlying architecture and its ability to model dependencies between words in a sentence.\", \"similarity\": 0.8, \"to\": \"XLNet\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"GPT-3 is a highly advanced language model that utilizes the Transformer architecture. It is similar to Transformer in terms of its ability to generate human-like text and perform various natural language processing tasks.\", \"similarity\": 0.75, \"to\": \"GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"T5 (Text-to-Text Transfer Transformer) is a language model that builds upon the Transformer architecture. It is similar to Transformer in terms of its ability to perform various text-to-text transfer tasks, such as translation, summarization, and question answering.\", \"similarity\": 0.7, \"to\": \"T5 (text-to-text transfer transformer)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"Both BERT and GPT are popular language models used for natural language processing tasks. They are both based on transformer architecture and have been trained on large amounts of text data.\", \"similarity\": 0.9, \"to\": \"OpenAI#GPT\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"ELMo and BERT are both contextual word embeddings models that capture word meanings based on their context in a sentence. They have been widely used for various NLP tasks and have significantly improved performance compared to traditional word embeddings.\", \"similarity\": 0.8, \"to\": \"ELMo\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"While Word2Vec is a word embedding model and BERT is a language model, they both aim to capture semantic relationships between words. Word2Vec represents words as dense vectors in a continuous vector space, while BERT captures contextualized word representations.\", \"similarity\": 0.7, \"to\": \"Word2vec\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"BERT and Transformer are closely related as BERT is built upon the transformer architecture. Transformers have revolutionized the field of NLP and have become the standard architecture for many language models, including BERT.\", \"similarity\": 0.9, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"ULMFiT and BERT are both popular language models that have significantly advanced the field of transfer learning in NLP. They have been trained on large-scale datasets and can be fine-tuned for specific downstream tasks.\", \"similarity\": 0.8, \"to\": \"ULMFiT\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT (language model)\", \"reason\": \"Both GPT and BERT are popular language models used for natural language processing tasks. They are both based on transformer architectures and have been trained on large amounts of text data.\", \"similarity\": 0.9, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT (language model)\", \"reason\": \"ELMo and GPT are both language models that have been trained on large corpora of text data. They both aim to capture contextual information and generate meaningful representations of words and sentences.\", \"similarity\": 0.8, \"to\": \"ELMo\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT (language model)\", \"reason\": \"OpenAI is the organization behind GPT. It is responsible for developing and releasing GPT models. Both GPT and OpenAI are closely related in terms of their development and usage.\", \"similarity\": 0.7, \"to\": \"OpenAI\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT (language model)\", \"reason\": \"Both GPT and the Transformer neural network architecture share a common foundation. GPT is built upon the Transformer architecture, which has revolutionized natural language processing tasks.\", \"similarity\": 0.9, \"to\": \"Transformer (neural network)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT (language model)\", \"reason\": \"GPT is a language model, and language modeling is the task it performs. Both GPT and language modeling are closely related concepts in the field of natural language processing.\", \"similarity\": 0.8, \"to\": \"Language modeling\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet (language model)\", \"reason\": \"Both XLNet and BERT are transformer-based language models that have achieved state-of-the-art performance on various natural language processing tasks.\", \"similarity\": 0.9, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet (language model)\", \"reason\": \"XLNet and GPT-3 are both advanced language models that utilize deep learning techniques and have demonstrated impressive language generation capabilities.\", \"similarity\": 0.8, \"to\": \"GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet (language model)\", \"reason\": \"RoBERTa is a variant of BERT, similar to XLNet, that has been pre-trained on a large corpus of text and has achieved state-of-the-art performance on various natural language understanding tasks.\", \"similarity\": 0.85, \"to\": \"RoBERTa\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet (language model)\", \"reason\": \"ALBERT is another variant of BERT, similar to XLNet, that aims to reduce model size and improve efficiency while maintaining high performance on various natural language processing tasks.\", \"similarity\": 0.85, \"to\": \"ALBERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet (language model)\", \"reason\": \"T5 is a versatile language model that can be fine-tuned for various natural language processing tasks, similar to XLNet. It has achieved state-of-the-art performance on several benchmarks.\", \"similarity\": 0.8, \"to\": \"T5 (text-to-text transfer transformer)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (language model)\", \"reason\": \"Both GPT-3 and T5 are state-of-the-art language models developed by OpenAI.\", \"similarity\": 0.9, \"to\": \"OpenAI#GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (language model)\", \"reason\": \"BERT is another popular language model that, like T5, is based on the Transformer architecture.\", \"similarity\": 0.8, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (language model)\", \"reason\": \"XLNet is a language model that, similar to T5, uses a permutation-based training objective.\", \"similarity\": 0.7, \"to\": \"XLNet\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (language model)\", \"reason\": \"RoBERTa is a variant of BERT that achieves state-of-the-art performance on various natural language processing tasks, similar to T5.\", \"similarity\": 0.75, \"to\": \"RoBERTa\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (language model)\", \"reason\": \"ALBERT is a lite version of BERT that reduces the model size while maintaining performance, similar to T5\\u0027s ability to perform various tasks.\", \"similarity\": 0.7, \"to\": \"ALBERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT series\", \"reason\": \"Both OpenAI\\u0027s GPT series and the Transformer model are based on the transformer architecture. The GPT series utilizes the transformer model for natural language processing tasks.\", \"similarity\": 0.9, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT series\", \"reason\": \"Both OpenAI\\u0027s GPT series and recurrent neural networks (RNNs) are used for sequential data processing. GPT models can be seen as a type of language model that captures dependencies between words in a sequence, similar to how RNNs capture dependencies between elements in a sequence.\", \"similarity\": 0.8, \"to\": \"Recurrent neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT series\", \"reason\": \"Both OpenAI\\u0027s GPT series and BERT are popular language models used for natural language processing tasks. While GPT models are autoregressive and generate text, BERT is a masked language model that learns to predict missing words in a sentence.\", \"similarity\": 0.7, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT series\", \"reason\": \"Both OpenAI\\u0027s GPT series and deep learning are related to the field of artificial intelligence. GPT models are deep learning models that utilize multiple layers of neural networks to process and generate text.\", \"similarity\": 0.6, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT series\", \"reason\": \"Both OpenAI\\u0027s GPT series and natural language processing (NLP) are concerned with the understanding and generation of human language. GPT models are widely used in NLP tasks such as language translation, text completion, and sentiment analysis.\", \"similarity\": 0.8, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet\", \"reason\": \"Both XLNet and BERT are transformer-based language models that use bidirectional context to generate representations for words.\", \"similarity\": 0.9, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet\", \"reason\": \"XLNet and GPT are both transformer-based language models that are pre-trained on large amounts of text data and can generate coherent and contextually relevant text.\", \"similarity\": 0.8, \"to\": \"GPT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet\", \"reason\": \"RoBERTa is an optimized version of BERT that addresses some of its limitations. Both XLNet and RoBERTa are transformer-based models that achieve state-of-the-art performance on various natural language processing tasks.\", \"similarity\": 0.7, \"to\": \"RoBERTa\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet\", \"reason\": \"ALBERT is a lite version of BERT that reduces the model size while maintaining performance. Both XLNet and ALBERT are transformer-based models that aim to improve efficiency and scalability.\", \"similarity\": 0.6, \"to\": \"ALBERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"XLNet\", \"reason\": \"T5 is a transformer-based model that can perform various natural language processing tasks by framing them as text-to-text transfer learning problems. XLNet and T5 share the transformer architecture and the ability to handle diverse NLP tasks.\", \"similarity\": 0.5, \"to\": \"T5 (text-to-text transfer transformer)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"GPT-2 is the predecessor to GPT-3 and shares similar architecture and capabilities.\", \"similarity\": 0.9, \"to\": \"OpenAI#GPT-2\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"GPT-3 is based on the Transformer model, which is a key component of its architecture.\", \"similarity\": 0.8, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"Both GPT-3 and NLP deal with understanding and generating human language.\", \"similarity\": 0.7, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"GPT-3 is an advanced AI model that demonstrates human-like language generation.\", \"similarity\": 0.6, \"to\": \"Artificial intelligence\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"GPT-3 utilizes deep learning techniques to achieve its language generation capabilities.\", \"similarity\": 0.7, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (text-to-text transfer transformer)\", \"reason\": \"T5 is a text-to-text transfer transformer model, and the Transformer model is the foundation of T5.\", \"similarity\": 0.9, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (text-to-text transfer transformer)\", \"reason\": \"Both T5 and BERT are transformer-based models used for natural language processing tasks.\", \"similarity\": 0.8, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (text-to-text transfer transformer)\", \"reason\": \"T5 and GPT are both transformer-based models used for natural language processing, although GPT is primarily used for generative tasks.\", \"similarity\": 0.7, \"to\": \"GPT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (text-to-text transfer transformer)\", \"reason\": \"XLNet is another transformer-based model that shares similarities with T5 in terms of architecture and natural language processing capabilities.\", \"similarity\": 0.6, \"to\": \"XLNet\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"T5 (text-to-text transfer transformer)\", \"reason\": \"RoBERTa is a variant of BERT that incorporates additional training techniques, similar to how T5 extends the capabilities of the base Transformer model.\", \"similarity\": 0.6, \"to\": \"RoBERTa\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT\", \"reason\": \"Both GPT and BERT are popular language models based on the Transformer architecture. They are designed to understand and generate human-like text.\", \"similarity\": 0.9, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT\", \"reason\": \"ELMo and GPT are both language models that use deep contextualized word representations. They capture the meaning of words based on their context in a sentence.\", \"similarity\": 0.8, \"to\": \"ELMo\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT\", \"reason\": \"ULMFiT and GPT are both language models that have been fine-tuned on large amounts of text data. They excel at various natural language processing tasks.\", \"similarity\": 0.7, \"to\": \"ULMFiT\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT\", \"reason\": \"XLNet and GPT are both language models that leverage the Transformer architecture. They have achieved state-of-the-art performance on various natural language processing benchmarks.\", \"similarity\": 0.85, \"to\": \"XLNet\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT\", \"reason\": \"RoBERTa and GPT are both language models that have been pre-trained on large amounts of text data. They have demonstrated strong performance on a wide range of natural language understanding tasks.\", \"similarity\": 0.75, \"to\": \"RoBERTa\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ELMo\", \"reason\": \"Both ELMo and BERT are contextual word embeddings models that capture word meaning based on their surrounding context.\", \"similarity\": 0.9, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ELMo\", \"reason\": \"ELMo and GPT-3 are both language models that generate text based on input prompts, with GPT-3 being a more advanced and powerful version.\", \"similarity\": 0.8, \"to\": \"OpenAI#GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ELMo\", \"reason\": \"Both ELMo and Word2Vec are word embedding models that represent words as dense vectors, capturing semantic meaning.\", \"similarity\": 0.7, \"to\": \"Word2vec\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ELMo\", \"reason\": \"ELMo and FastText are both word embedding models that consider subword information to capture morphological and semantic meaning.\", \"similarity\": 0.6, \"to\": \"FastText\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ELMo\", \"reason\": \"ELMo and ULMFiT are both language models that use transfer learning techniques to improve performance on downstream NLP tasks.\", \"similarity\": 0.5, \"to\": \"ULMFiT\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Word2vec\", \"reason\": \"Both Word2vec and GloVe are popular algorithms used for word embeddings in natural language processing tasks.\", \"similarity\": 0.9, \"to\": \"GloVe (machine learning)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Word2vec\", \"reason\": \"FastText is another word embedding algorithm that extends Word2vec by considering subword information. Both algorithms are widely used for word representation learning.\", \"similarity\": 0.8, \"to\": \"FastText\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Word2vec\", \"reason\": \"ELMo is a deep contextualized word representation model that captures word meaning based on its context. Like Word2vec, it aims to provide meaningful word embeddings.\", \"similarity\": 0.7, \"to\": \"ELMo\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Word2vec\", \"reason\": \"BERT is a transformer-based language model that generates contextualized word embeddings. It shares the goal of Word2vec in providing word representations that capture semantic meaning.\", \"similarity\": 0.85, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Word2vec\", \"reason\": \"Doc2vec is an extension of Word2vec that learns document-level embeddings. Both algorithms are part of the same family and share similar principles.\", \"similarity\": 0.75, \"to\": \"Doc2vec\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ULMFiT\", \"reason\": \"Both ULMFiT and BERT are popular language models used for natural language processing tasks. They both employ transformer architectures and pre-training techniques.\", \"similarity\": 0.8, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ULMFiT\", \"reason\": \"ULMFiT and ELMo are both language models that generate contextual word embeddings. They have been influential in advancing natural language understanding tasks.\", \"similarity\": 0.7, \"to\": \"ELMo\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ULMFiT\", \"reason\": \"ULMFiT and GPT are language models that utilize transformer architectures and pre-training techniques. They have contributed to significant advancements in natural language generation tasks.\", \"similarity\": 0.75, \"to\": \"OpenAI#GPT series\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ULMFiT\", \"reason\": \"ULMFiT and Word2Vec are both popular models used for word embeddings. They aim to capture semantic relationships between words and have been widely adopted in various natural language processing applications.\", \"similarity\": 0.6, \"to\": \"Word2vec\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ULMFiT\", \"reason\": \"ULMFiT and FastText are language models that focus on efficient word representations. They both incorporate subword information and have been successful in various natural language processing tasks.\", \"similarity\": 0.65, \"to\": \"FastText\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI\", \"reason\": \"OpenAI is an organization that focuses on artificial intelligence research and development.\", \"similarity\": 0.9, \"to\": \"Artificial intelligence\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI\", \"reason\": \"OpenAI heavily utilizes machine learning techniques in their research and applications.\", \"similarity\": 0.8, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI\", \"reason\": \"OpenAI works on developing models and algorithms for natural language processing tasks.\", \"similarity\": 0.7, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI\", \"reason\": \"OpenAI has made significant contributions to the field of reinforcement learning.\", \"similarity\": 0.6, \"to\": \"Reinforcement learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI\", \"reason\": \"OpenAI utilizes deep learning models and architectures in their research and applications.\", \"similarity\": 0.6, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (neural network)\", \"reason\": \"Both GPT-3 and Transformer are state-of-the-art language models based on the Transformer architecture.\", \"similarity\": 0.9, \"to\": \"GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (neural network)\", \"reason\": \"BERT is another popular language model that utilizes the Transformer architecture.\", \"similarity\": 0.8, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (neural network)\", \"reason\": \"XLNet is a language model that builds upon the Transformer architecture, similar to Transformer (neural network).\", \"similarity\": 0.7, \"to\": \"XLNet\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (neural network)\", \"reason\": \"RoBERTa is a variant of BERT that also employs the Transformer architecture.\", \"similarity\": 0.7, \"to\": \"RoBERTa\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (neural network)\", \"reason\": \"ALBERT is a lite version of BERT that shares similarities with Transformer (neural network).\", \"similarity\": 0.6, \"to\": \"ALBERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Language modeling\", \"reason\": \"Language modeling is a subfield of natural language processing that focuses on predicting the next word or sequence of words in a given context.\", \"similarity\": 0.9, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Language modeling\", \"reason\": \"Language modeling is a statistical approach to language processing, where probabilities are assigned to sequences of words.\", \"similarity\": 0.8, \"to\": \"Statistical language modeling\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Language modeling\", \"reason\": \"Language modeling is used in machine translation to generate fluent and coherent translations by predicting the most likely words or phrases in the target language.\", \"similarity\": 0.7, \"to\": \"Machine translation\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Language modeling\", \"reason\": \"Language modeling is employed in speech recognition systems to improve accuracy by predicting the most likely words or phrases based on the acoustic input.\", \"similarity\": 0.6, \"to\": \"Speech recognition\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Language modeling\", \"reason\": \"Language modeling is used in information retrieval systems to rank documents based on their relevance to a given query, by estimating the probability of the query given the document.\", \"similarity\": 0.5, \"to\": \"Information retrieval\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"RoBERTa\", \"reason\": \"Both RoBERTa and BERT are transformer-based language models that use the same architecture and pre-training objectives.\", \"similarity\": 0.9, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"RoBERTa\", \"reason\": \"RoBERTa and GPT-3 are both state-of-the-art language models that have achieved impressive results in various natural language processing tasks.\", \"similarity\": 0.8, \"to\": \"GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"RoBERTa\", \"reason\": \"RoBERTa and ELMo are both contextual word embeddings models that capture word meaning based on their surrounding context.\", \"similarity\": 0.7, \"to\": \"ELMo\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"RoBERTa\", \"reason\": \"RoBERTa and ULMFiT are both transfer learning approaches for natural language processing tasks, aiming to improve performance on downstream tasks by pre-training on large corpora.\", \"similarity\": 0.6, \"to\": \"ULMFiT\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"RoBERTa\", \"reason\": \"RoBERTa and XLNet are both transformer-based language models that have achieved state-of-the-art results on various natural language processing benchmarks.\", \"similarity\": 0.85, \"to\": \"XLNet\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ALBERT (language model)\", \"reason\": \"BERT is a language model like ALBERT, both models are based on the Transformer architecture and have been pre-trained on large amounts of text data.\", \"similarity\": 0.9, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ALBERT (language model)\", \"reason\": \"GPT-3 is another language model that shares similarities with ALBERT. Both models are designed to generate human-like text and have been trained on massive amounts of data.\", \"similarity\": 0.8, \"to\": \"GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ALBERT (language model)\", \"reason\": \"ELMo is a language model that, like ALBERT, utilizes deep contextualized word representations. Both models aim to capture the meaning of words in context.\", \"similarity\": 0.7, \"to\": \"ELMo\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ALBERT (language model)\", \"reason\": \"ULMFiT is a language model that, similar to ALBERT, focuses on transfer learning for natural language processing tasks. Both models aim to improve performance on downstream tasks by leveraging pre-training.\", \"similarity\": 0.6, \"to\": \"ULMFiT\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"ALBERT (language model)\", \"reason\": \"RoBERTa is a language model that shares similarities with ALBERT. Both models are based on the Transformer architecture and have been trained on large-scale corpora to learn contextual representations of words.\", \"similarity\": 0.85, \"to\": \"RoBERTa\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT-3\", \"reason\": \"Both GPT-2 and GPT-3 are language models developed by OpenAI.\", \"similarity\": 0.9, \"to\": \"OpenAI#GPT-2\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT-3\", \"reason\": \"Both GPT-3 and the Transformer model are based on the transformer architecture.\", \"similarity\": 0.8, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT-3\", \"reason\": \"GPT-3 is a language model used for natural language processing tasks.\", \"similarity\": 0.7, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT-3\", \"reason\": \"GPT-3 is an example of artificial intelligence technology.\", \"similarity\": 0.6, \"to\": \"Artificial intelligence\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"OpenAI#GPT-3\", \"reason\": \"GPT-3 is built using deep learning techniques.\", \"similarity\": 0.7, \"to\": \"Deep learning\", \"width\": 1}]);\n",
              "\n",
              "                  nodeColors = {};\n",
              "                  allNodes = nodes.get({ returnType: \"Object\" });\n",
              "                  for (nodeId in allNodes) {\n",
              "                    nodeColors[nodeId] = allNodes[nodeId].color;\n",
              "                  }\n",
              "                  allEdges = edges.get({ returnType: \"Object\" });\n",
              "                  // adding nodes and edges to the graph\n",
              "                  data = {nodes: nodes, edges: edges};\n",
              "\n",
              "                  var options = {\n",
              "    \"configure\": {\n",
              "        \"enabled\": false\n",
              "    },\n",
              "    \"edges\": {\n",
              "        \"color\": {\n",
              "            \"inherit\": true\n",
              "        },\n",
              "        \"smooth\": {\n",
              "            \"enabled\": true,\n",
              "            \"type\": \"dynamic\"\n",
              "        }\n",
              "    },\n",
              "    \"interaction\": {\n",
              "        \"dragNodes\": true,\n",
              "        \"hideEdgesOnDrag\": false,\n",
              "        \"hideNodesOnDrag\": false\n",
              "    },\n",
              "    \"physics\": {\n",
              "        \"enabled\": true,\n",
              "        \"forceAtlas2Based\": {\n",
              "            \"avoidOverlap\": 0,\n",
              "            \"centralGravity\": 0.01,\n",
              "            \"damping\": 0.4,\n",
              "            \"gravitationalConstant\": -50,\n",
              "            \"springConstant\": 0.03,\n",
              "            \"springLength\": 100\n",
              "        },\n",
              "        \"solver\": \"forceAtlas2Based\",\n",
              "        \"stabilization\": {\n",
              "            \"enabled\": true,\n",
              "            \"fit\": true,\n",
              "            \"iterations\": 1000,\n",
              "            \"onlyDynamicEdges\": false,\n",
              "            \"updateInterval\": 50\n",
              "        }\n",
              "    }\n",
              "};\n",
              "\n",
              "                  \n",
              "\n",
              "\n",
              "                  \n",
              "\n",
              "                  network = new vis.Network(container, data, options);\n",
              "\n",
              "                  \n",
              "\n",
              "                  \n",
              "\n",
              "                  \n",
              "                  // make a custom popup\n",
              "                      var popup = document.createElement(\"div\");\n",
              "                      popup.className = 'popup';\n",
              "                      popupTimeout = null;\n",
              "                      popup.addEventListener('mouseover', function () {\n",
              "                          console.log(popup)\n",
              "                          if (popupTimeout !== null) {\n",
              "                              clearTimeout(popupTimeout);\n",
              "                              popupTimeout = null;\n",
              "                          }\n",
              "                      });\n",
              "                      popup.addEventListener('mouseout', function () {\n",
              "                          if (popupTimeout === null) {\n",
              "                              hidePopup();\n",
              "                          }\n",
              "                      });\n",
              "                      container.appendChild(popup);\n",
              "\n",
              "\n",
              "                      // use the popup event to show\n",
              "                      network.on(\"showPopup\", function (params) {\n",
              "                          showPopup(params);\n",
              "                      });\n",
              "\n",
              "                      // use the hide event to hide it\n",
              "                      network.on(\"hidePopup\", function (params) {\n",
              "                          hidePopup();\n",
              "                      });\n",
              "\n",
              "                      // hiding the popup through css\n",
              "                      function hidePopup() {\n",
              "                          popupTimeout = setTimeout(function () { popup.style.display = 'none'; }, 500);\n",
              "                      }\n",
              "\n",
              "                      // showing the popup\n",
              "                      function showPopup(nodeId) {\n",
              "                          // get the data from the vis.DataSet\n",
              "                          var nodeData = nodes.get([nodeId]);\n",
              "                          popup.innerHTML = nodeData[0].title;\n",
              "\n",
              "                          // get the position of the node\n",
              "                          var posCanvas = network.getPositions([nodeId])[nodeId];\n",
              "\n",
              "                          // get the bounding box of the node\n",
              "                          var boundingBox = network.getBoundingBox(nodeId);\n",
              "\n",
              "                          //position tooltip:\n",
              "                          posCanvas.x = posCanvas.x + 0.5 * (boundingBox.right - boundingBox.left);\n",
              "\n",
              "                          // convert coordinates to the DOM space\n",
              "                          var posDOM = network.canvasToDOM(posCanvas);\n",
              "\n",
              "                          // Give it an offset\n",
              "                          posDOM.x += 10;\n",
              "                          posDOM.y -= 20;\n",
              "\n",
              "                          // show and place the tooltip.\n",
              "                          popup.style.display = 'block';\n",
              "                          popup.style.top = posDOM.y + 'px';\n",
              "                          popup.style.left = posDOM.x + 'px';\n",
              "                      }\n",
              "                  \n",
              "\n",
              "\n",
              "                  \n",
              "\n",
              "                  return network;\n",
              "\n",
              "              }\n",
              "              drawGraph();\n",
              "        </script>\n",
              "    </body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DDdCVOn4lV-Q"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}
